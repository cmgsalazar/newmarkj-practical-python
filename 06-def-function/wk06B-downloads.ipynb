{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja871rAui_Da"
   },
   "source": [
    "# Scraping Files from Websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwHrS-KJi_Dg"
   },
   "source": [
    "### You need to create a data set that tracks how many companies the <a href=\"https://www.sec.gov/litigation/suspensions.shtml\">SEC suspended</a> between 2019 and 1999. You find the data at:\n",
    "\n",
    "```https://www.sec.gov/litigation/suspensions.shtml```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_OAOaqLi_Dh"
   },
   "source": [
    "### We want to write a scraper that aggregates:\n",
    "\n",
    "* Date of suspension\n",
    "* Company name\n",
    "* Order\n",
    "* Release (the PDFs in the XX-YYYYY format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECK4Q7IOi_Di"
   },
   "source": [
    "# The Challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfkXDlQvi_Di"
   },
   "source": [
    "# Downloading files from websites \n",
    "\n",
    "Having to download files from a site that holds target data is one of the most common types of scrapes:\n",
    "\n",
    "- <a href=\"https://www.uscourts.gov/report-name/bankruptcy-filings\">Bankruptcy documents</a>\n",
    "- <a href=\"https://apps.health.ny.gov/pubdoh/professionals/doctors/conduct/factions/AllRecordsAction.action\">NYS Doctors Displinary Decisions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W1jQ2nQei_Dj"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup  ## scrape info from web pages\n",
    "import requests ## get web pages from server\n",
    "import time # time is required. we will use its sleep function\n",
    "from random import randrange # generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape url to soup\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/pages.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z42CN1eKi_Dk"
   },
   "source": [
    "## Find all PDFs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save target data in list\n",
    "aTags = soup.find(\"ul\", class_=\"pdfs\").find_all(\"a\")\n",
    "aTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what type\n",
    "type(aTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save urls in a list using lc\n",
    "links = [ base_url + aTag.get(\"href\") for aTag in aTags ]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing new library\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling snoozer timer from class exercise\n",
    "from random import randrange\n",
    "import time\n",
    "def snoozer(start_time, end_time):\n",
    "    '''\n",
    "    This function creates a snoozer that can be used when scraping.\n",
    "    para1 = start time of range, in seconds\n",
    "    para2 = end time of range, in seconds\n",
    "    '''\n",
    "    timer = randrange(start_time, end_time)\n",
    "    print(f\"Snoozing for {timer} seconds...\")\n",
    "    time.sleep(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 10.\n",
      "100% [..........................................................] 12812 / 12812\n",
      "Snoozing for 12 seconds...\n",
      "Downloading link 2 of 10.\n",
      "100% [..........................................................] 12897 / 12897\n",
      "Snoozing for 18 seconds...\n",
      "Downloading link 3 of 10.\n",
      "100% [..........................................................] 12908 / 12908\n",
      "Snoozing for 15 seconds...\n",
      "Downloading link 4 of 10.\n",
      "100% [..........................................................] 12843 / 12843\n",
      "Snoozing for 16 seconds...\n",
      "Downloading link 5 of 10.\n",
      "100% [..........................................................] 12881 / 12881\n",
      "Snoozing for 20 seconds...\n",
      "Downloading link 6 of 10.\n",
      "100% [..........................................................] 12906 / 12906\n",
      "Snoozing for 16 seconds...\n",
      "Downloading link 7 of 10.\n",
      "100% [..........................................................] 12816 / 12816\n",
      "Snoozing for 14 seconds...\n",
      "Downloading link 8 of 10.\n",
      "100% [..........................................................] 12921 / 12921\n",
      "Snoozing for 18 seconds...\n",
      "Downloading link 9 of 10.\n",
      "100% [..........................................................] 12901 / 12901\n",
      "Snoozing for 16 seconds...\n",
      "Downloading link 10 of 10.\n",
      "100% [..........................................................] 13049 / 13049\n",
      "Snoozing for 18 seconds...\n",
      "Done downloading 10 of 10.\n"
     ]
    }
   ],
   "source": [
    "# download with timer\n",
    "link_count = 0\n",
    "start_range, end_range = 10, 21\n",
    "\n",
    "for link in links:\n",
    "    link_count += 1\n",
    "    print(f\"Downloading link {link_count} of {len(links)}.\")\n",
    "    ## downloading docs\n",
    "    wget.download(link)\n",
    "    print(\"\")\n",
    "    snoozer(start_range, end_range)\n",
    "print(f\"Done downloading {link_count} of {len(links)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn - Download the first 10 text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "week-7-download_docs_BLANK.ipynb",
   "provenance": [
    {
     "file_id": "1nqLLwMBVA4UJus95SFvmBL-GsFXxpcsW",
     "timestamp": 1638974922793
    },
    {
     "file_id": "1J1lhUQakLF4NmWxBeEGiprUMSv1j7cpk",
     "timestamp": 1628019785208
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
