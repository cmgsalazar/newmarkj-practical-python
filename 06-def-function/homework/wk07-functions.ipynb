{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5320cbbf",
   "metadata": {},
   "source": [
    "# Homework for Week 7\n",
    "## Functions and Downloading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5db5b3",
   "metadata": {},
   "source": [
    "### Write modular functions for the following:\n",
    "\n",
    "1. Making a ```request```\n",
    "2. Converting a ```response``` into ```soup```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca72522-9a5c-41e7-b5a3-86b3fa29e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b96b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. code here for requests function (create cells if/as needed)\n",
    "\n",
    "def fetchURL(url):\n",
    "    '''\n",
    "    This function checks if a webpage is active, gets the source code of a webpage, then turns it into text.\n",
    "    parameter (url): full link of the webpage\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(f\"The webpage has an error: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed087ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. code here for response function (create cells if/as needed)\n",
    "\n",
    "def parseURL(response):\n",
    "    '''\n",
    "    This function parses the `response` for scraping.\n",
    "    parameter (response): response = requests.get(url)\n",
    "    '''\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350b62c",
   "metadata": {},
   "source": [
    "### 3. Demo downloading files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files <a href=\"https://sandeepmj.github.io/scrape-example-page/pages.html\">on this site</a>. During class we downloaded on e set of text files and one set of PDF files.\n",
    "\n",
    "Now download **ALL files at one time**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a209478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here (create cells if/as needed)\n",
    "\n",
    "response = fetchURL(\"https://sandeepmj.github.io/scrape-example-page/pages.html\")\n",
    "soup = parseURL(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18576596-0624-4a6b-87b8-c485688a0151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this first set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       "</ul>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finds container of files to download\n",
    "\n",
    "ul_tags = soup.find_all(\"ul\", class_=\"downloadable\")\n",
    "ul_tags[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d9e6ca-bfb0-4506-bae6-fde20e50d847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_02.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_03.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_04.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_05.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_06.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_07.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_08.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_09.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_10.txt\">10</a>],\n",
       " [<a href=\"files/pdf_1.pdf\">1</a>,\n",
       "  <a href=\"files/pdf_2.pdf\">2</a>,\n",
       "  <a href=\"files/pdf_3.pdf\">3</a>,\n",
       "  <a href=\"files/pdf_4.pdf\">4</a>,\n",
       "  <a href=\"files/pdf_5.pdf\">5</a>,\n",
       "  <a href=\"files/pdf_6.pdf\">6</a>,\n",
       "  <a href=\"files/pdf_7.pdf\">7</a>,\n",
       "  <a href=\"files/pdf_8.pdf\">8</a>,\n",
       "  <a href=\"files/pdf_9.pdf\">9</a>,\n",
       "  <a href=\"files/pdf_10.pdf\">10</a>],\n",
       " [<a href=\"files/text_doc_A.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_B.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_C.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_D.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_E.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_F.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_G.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_H.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_I.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_J.txt\">10</a>]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate to get all the links\n",
    "\n",
    "a_tags = [ ul.find_all(\"a\") for ul in ul_tags ]\n",
    "a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5154d062-5d27-4c42-bfff-30de31e19a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>,\n",
       " <a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>,\n",
       " <a href=\"files/text_doc_A.txt\">1</a>,\n",
       " <a href=\"files/text_doc_B.txt\">2</a>,\n",
       " <a href=\"files/text_doc_C.txt\">3</a>,\n",
       " <a href=\"files/text_doc_D.txt\">4</a>,\n",
       " <a href=\"files/text_doc_E.txt\">5</a>,\n",
       " <a href=\"files/text_doc_F.txt\">6</a>,\n",
       " <a href=\"files/text_doc_G.txt\">7</a>,\n",
       " <a href=\"files/text_doc_H.txt\">8</a>,\n",
       " <a href=\"files/text_doc_I.txt\">9</a>,\n",
       " <a href=\"files/text_doc_J.txt\">10</a>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten list\n",
    "import itertools\n",
    "\n",
    "downloads_list = list(itertools.chain(*a_tags))\n",
    "downloads_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c981e94d-980a-46e0-958f-db4e5f2016a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_A.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_B.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_C.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_D.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_E.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_F.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_G.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_H.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_I.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_J.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list to hold all download links\n",
    "\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\"\n",
    "download_links = [ base_url + download.get(\"href\") for download in downloads_list ]\n",
    "download_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1df6c0-f51e-4174-9625-951f5baea101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading files... importing libraries first\n",
    "\n",
    "from random import randrange\n",
    "import time\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6d1f3e-c290-4a1a-a0d0-bd74db627276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new function for snoozer \n",
    "# pulled from class exercise\n",
    "\n",
    "def snoozer(start_time, end_time):\n",
    "    '''\n",
    "    This function creates a snoozer that can be used when scraping.\n",
    "    parameter 1: start time of range, in seconds\n",
    "    parameter 2: end time of range, in seconds\n",
    "    '''\n",
    "    timer = randrange(start_time, end_time)\n",
    "    print(f\"Snoozing for {timer} seconds...\")\n",
    "    time.sleep(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f584d8a4-d9e4-4030-8cca-70aa888a6357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 30.\n",
      "100% [................................................................] 76 / 76\n",
      "Snoozing for 14 seconds...\n",
      "Downloading link 2 of 30.\n",
      "100% [................................................................] 66 / 66\n",
      "Snoozing for 7 seconds...\n",
      "Downloading link 3 of 30.\n",
      "100% [................................................................] 70 / 70\n",
      "Snoozing for 12 seconds...\n",
      "Downloading link 4 of 30.\n",
      "100% [................................................................] 63 / 63\n",
      "Snoozing for 15 seconds...\n",
      "Downloading link 5 of 30.\n",
      "100% [................................................................] 66 / 66\n",
      "Snoozing for 10 seconds...\n",
      "Downloading link 6 of 30.\n",
      "100% [................................................................] 66 / 66\n",
      "Snoozing for 7 seconds...\n",
      "Downloading link 7 of 30.\n",
      "100% [................................................................] 69 / 69\n",
      "Snoozing for 9 seconds...\n",
      "Downloading link 8 of 30.\n",
      "100% [................................................................] 65 / 65\n",
      "Snoozing for 11 seconds...\n",
      "Downloading link 9 of 30.\n",
      "100% [................................................................] 66 / 66\n",
      "Snoozing for 5 seconds...\n",
      "Downloading link 10 of 30.\n",
      "100% [................................................................] 60 / 60\n",
      "Snoozing for 9 seconds...\n",
      "Downloading link 11 of 30.\n",
      "100% [..........................................................] 12812 / 12812\n",
      "Snoozing for 11 seconds...\n",
      "Downloading link 12 of 30.\n",
      "100% [..........................................................] 12897 / 12897\n",
      "Snoozing for 12 seconds...\n",
      "Downloading link 13 of 30.\n",
      "100% [..........................................................] 12908 / 12908\n",
      "Snoozing for 6 seconds...\n",
      "Downloading link 14 of 30.\n",
      "100% [..........................................................] 12843 / 12843\n",
      "Snoozing for 7 seconds...\n",
      "Downloading link 15 of 30.\n",
      "100% [..........................................................] 12881 / 12881\n",
      "Snoozing for 7 seconds...\n",
      "Downloading link 16 of 30.\n",
      "100% [..........................................................] 12906 / 12906\n",
      "Snoozing for 15 seconds...\n",
      "Downloading link 17 of 30.\n",
      "100% [..........................................................] 12816 / 12816\n",
      "Snoozing for 11 seconds...\n",
      "Downloading link 18 of 30.\n",
      "100% [..........................................................] 12921 / 12921\n",
      "Snoozing for 6 seconds...\n",
      "Downloading link 19 of 30.\n",
      "100% [..........................................................] 12901 / 12901\n",
      "Snoozing for 10 seconds...\n",
      "Downloading link 20 of 30.\n",
      "100% [..........................................................] 13049 / 13049\n",
      "Snoozing for 14 seconds...\n",
      "Downloading link 21 of 30.\n",
      "100% [................................................................] 69 / 69\n",
      "Snoozing for 11 seconds...\n",
      "Downloading link 22 of 30.\n",
      "100% [................................................................] 61 / 61\n",
      "Snoozing for 12 seconds...\n",
      "Downloading link 23 of 30.\n",
      "100% [................................................................] 72 / 72\n",
      "Snoozing for 15 seconds...\n",
      "Downloading link 24 of 30.\n",
      "100% [................................................................] 68 / 68\n",
      "Snoozing for 7 seconds...\n",
      "Downloading link 25 of 30.\n",
      "100% [................................................................] 70 / 70\n",
      "Snoozing for 12 seconds...\n",
      "Downloading link 26 of 30.\n",
      "100% [................................................................] 66 / 66\n",
      "Snoozing for 6 seconds...\n",
      "Downloading link 27 of 30.\n",
      "100% [................................................................] 76 / 76\n",
      "Snoozing for 5 seconds...\n",
      "Downloading link 28 of 30.\n",
      "100% [................................................................] 71 / 71\n",
      "Snoozing for 5 seconds...\n",
      "Downloading link 29 of 30.\n",
      "100% [................................................................] 63 / 63\n",
      "Snoozing for 5 seconds...\n",
      "Downloading link 30 of 30.\n",
      "100% [................................................................] 71 / 71\n",
      "Snoozing for 12 seconds...\n",
      "Done downloading 30 of 30.\n"
     ]
    }
   ],
   "source": [
    "# downloading files... for real, this time\n",
    "\n",
    "counter = 0\n",
    "start_range, end_range = 5, 16\n",
    "\n",
    "for link in download_links:\n",
    "    counter += 1\n",
    "    print(f\"Downloading link {counter} of {len(download_links)}.\")\n",
    "    wget.download(link)\n",
    "    print(\"\")\n",
    "    snoozer(start_range, end_range)\n",
    "print(f\"Done downloading {counter} of {len(download_links)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cc723-b3dc-40be-9039-127c5ff9b1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
